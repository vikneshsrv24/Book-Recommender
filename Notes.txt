We can use API or large book dataset for the project.
Popular API includes :
 * ISBNdb - Cost [https://isbndb.com/isbn-database]
 * Google Books - Free [https://developers.google.com/books/docs/v1/using]
 * Open Library - Free [https://openlibrary.org/dev/docs/api/books]

Here we are using goodreads dataset.
 1- First dataset contains user_id - each user of goodreads.
 2- Book_id each id for books in the datasets.
 3- Ratings of each books

 Next dataset we have book metadata which contains
 informations like series, country code, lang code,
 description,genre,publication year etc.

 Third dataset includes book_id because it helps us
 in connecting the same book from first dataset

Three main steps in our projects are,
   1 - Search for Books
   2 - Create Book Lists
   3 - Recommend Books

We will also going to build a search engine for searching books

goodreads dataset website [https://mengtingwan.github.io/data/goodreads#datasets]

-> goodreads_interactions.csv [https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/goodreads_interactions.csv] - 4 GB
-> goodreads_books.json.gz [https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/goodreads_books.json.gz] - 2 GB
-> book_id_map.csv - [https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/book_id_map.csv] - 36 MB

As we have file size of more than 2 GB we cannot
upload directly in github. Instead we can able to upload
only via git lfs.
 * Git lfs - [https://git-lfs.com/]

To upload via terminal, Refer - [https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage]
 
